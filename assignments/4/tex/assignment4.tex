\documentclass[11pt,a4paper]{article}
% See geometry.pdf to learn the layout options. There are lots.
\usepackage{geometry}
\geometry{left = 3cm, right = 3cm, top = 3cm, bottom = 3cm}
% Activate to begin paragraphs with an empty line rather than an indent
\usepackage[parfill]{parskip}
\usepackage[table]{xcolor}
\usepackage{courier,multirow,fancyhdr}
\usepackage{float,amsmath,graphicx,framed,subfiles}
\usepackage{amssymb,subcaption,textcomp,listings}
\usepackage{booktabs,epstopdf,caption,units}
\usepackage{tabularx}
% hyperref should be loaded last
\usepackage[
    colorlinks=true,
    pdfencoding=auto,
    pdfauthor={Oliver Lee},
    pdftitle={WB2301-5 System Identification and Parameter Estimation:
    Assignment 4 - Optimization & Real Data Analysis},
    pdftex
]{hyperref}

\lstset{
language=Matlab,
basicstyle=\footnotesize\ttfamily,
showspaces=false,
showstringspaces=false,
showtabs=false,
frame=single,
tabsize=2,
captionpos=b,
breaklines=false,
breakatwhitespace=false,
numbers=left,
numbersep=4pt,
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\frenchspacing

\pagestyle{fancy}
\fancyhead{}
\fancyhead[R]{Lee}
\fancyhead[L]{Assignment 4}


\title{WB2301-5 System Identification and Parameter Estimation \\
Assignment 4 - Optimization \& Real Data Analysis}
\author{Oliver Lee}
\date{\today}
\graphicspath{ {images/} }

\begin{document}
\maketitle

\section{Optimization Techniques}

% 1a
\subsection{Optimization with Grid and Gradient Search}
Using a model $ \hat{y}(t) $ assumed to be:
\begin{equation*}
    \hat{y}(t) = a\cos(\frac{bt}{2}) + b\sin(\frac{at}{2})
\end{equation*}
and parameters $ a,\: b \in [0, 10] $. To find the optimal values of $a$ and $b$,
the error function $e$ to minimize is defined as:
\begin{equation*}
    e = (y(t) - \hat{y}(t))^T (y(t) - \hat{y}(t))
\end{equation*}
where $y(t),\: \hat{y}(t) \in \mathbb{R}^{n \times 1} $ and $n$ is the number
of elements of the considered time vector.

\autoref{fig:optsurf} shows the error calculation for parameters $a, b$ with a
resolution of $0.1$ (a 2-d grid) as well as multiple pathways from the use of
gradient search with different initial conditions. All initial conditions are
chosen randomly, except for the given condition $a = 5, \: b = 5$. As can be
seen, poor choices in initial conditions result in convergence at local minima
not equal to the global minimum. In fact, for the given problem and the
randomly chosen initial conditions, most do not converge at the global minimum.
If the error function surface (or hypersurface for a general optimization
problem) is not known a priori (i.e. grid search was not performed before use
of gradient search), it is possible to execute a number of gradient searches
which only return a local minimum without knowing that the global minimum
exists elsewhere in the search space.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{optsurf.eps}
    \caption{Error Function Surface with Gradient Search Pathways}
    \label{fig:optsurf}
\end{figure}
% Given a general model structure of:
% \begin{equation}
%     y(t) = G(q)u(t) + H(q)e(t) \label{eq:gen_model}
% \end{equation}
% where $y(t)$ is the output signal, $u(t)$ is the input signal, $e(t)$ is noise,
% and $H(q)$ represents the noise dynamics and $G(q)$ represents the system
% dynamics. From the system, we can express $y(t)$ in terms of $n(t)$ and $u(t)$:
% \begin{equation}
%     y(t)= H(q)u(t) + n(t) \label{eq:y1}
% \end{equation}
% It can be shown that the expressions for $u(t)$ and $y(t)$ are
% given by the following expressions:
% \begin{align}
%     u(t) &= \frac{1}{1 + H(q)G(q)}r(t) -
%         \frac{G(q)}{1 + H(q)G(q)}n(t) \label{eq:u} \\
%     y(t) &= \frac{H(q)}{1 + H(q)G(q)}r(t) +
%         \frac{1}{1 + H(q)G(q)}n(t) \label{eq:y2}
% \end{align}

% 1b
\subsection{Advantages and Disadvantages of Various Optimization Techniques}
\begin{table}
    \centering
    \begin{tabular}{|r|r|r|r|r|}
        \hline
%        \multicolumn{5}{|c|}{Performance of Optimization Techniques} \\
%        \hline
        \nonumber & \shortstack[c]{estimated\\parameters (a, b)} &
            \shortstack[c]{error\\residual} &
            \shortstack[c]{number of\\iterations} &
            \shortstack[c]{calculation\\time} \\
        \hline
        Grid Search & $ (5.8,\: 3.5) $ & $ 8.4835 $ & $ 10201 $ &
            $ 0.9455 $ s \\
        Gradient Search & $ (5.97,\: 3.53) $ & $ 5.3066 \times 10^{-17} $ &
            $ 8 $ & $ 0.6253 $ s \\
        Genetic Algorithm & $ (5.97,\: 3.53) $ & $ 2.2417 \times 10^{-7} $ &
            $ 100 $ & $ 4.4695 $ s \\
        \hline
    \end{tabular}
    \caption{Performance of Optimization Techniques}
    \label{tab:optcomp}
\end{table}
In \autoref{tab:optcomp} gradient search uses an initial condition of $ (5,\:
5) $. Performance of gradient search with different initial conditions will
result in small variations of estimated parameters, error residual, number of
iterations, and calculation time unless the initial condition is poorly choosen
and the search finds a different local minimum. Perfomance in the genetic
algorithm (GA) will also vary slightly in difference instances due to
randomness associated with genetic operations although the perfomance values
can be expected to be of the same order. Refer to \autoref{fig:optsurf} for
examples of poorly chosen initial conditions resulting in a different local
minimum with gradient search. \autoref{tab:optcomp} shows that grid search
results in a large number of searches and does find the global minimum,
although the precision of the estimated parameters is limited by the resolution
of the grid used. Both gradient search and the genetic algorithm find the same
minimum, with higher precision. Gradient search is able to do quickly, with
fewest function iterations and lowest computation time. The GA is also able to
find the global minimum is fewer function iterations than grid search (orders
of magnitude lower) but total computation time is higher due to creation of
candidate solutions and selection/genetic operations used in creating
subsequent generations. Although the GA optimization takes longer in this
problem, larger and more complex optimization problems may have jacobian and
hessian matrices that are more expensive to compute (in time and memory) for
gradient based methods and GA may faster in some cases.

Advantages and disadvantages for the three optimization techniques can be
summarized below:
\begin{itemize}
    \item Grid Search
        \begin{itemize}
            \item Will determine global minimum (with given grid resolution)
            \item Astronomical number of function evalutaions (dependent on
                grid resolution)
        \end{itemize}
    \item Gradient Search
        \begin{itemize}
            \item Very fast convergence near optimum
            \item Dependent on initial conditions
        \end{itemize}
    \item Genetic Algorithm
        \begin{itemize}
            \item Does not need to compute function gradients
            \item Slow convergence near optimum
        \end{itemize}
\end{itemize}

% 1c
\subsection{``Improvements'' for Grid Search}

% 1d
\subsection{Parameters of Genetic  Algorithms}

% 1e
\subsection{Combination of Grid and Gradient Search}
% FIXME: How is a "function evaluation" defined? Is it a single evaluation
% of the output? What about calculation of the Jacobian and Hessian?

\section{Real Data Analysis}
% FIXME: What movie? WriteExperiment.3GP is of low resolution and you can't see
% the monitor.

% 2a
\subsection{Figure 1?}

% 2b
\subsection{Coherence of Whole Dataset}

% 2c
\subsection{Validity of LTI identification}

% 2d
\subsection{Dynamic Characteristics and Parameter Estimation}
A mass-spring-damper system affects the frequency response most significantly
near the natural frequency of the system $\omega_n$, which is given by:
\begin{equation}
    \omega_n = \sqrt{\frac{k}{m}}
\end{equation}
Note that the damping of the system does not affect the natural frequency of
the system.

% 2e
\subsection{Estimation of Mass, Spring, Damper Parameters}

% 2f
\subsection{Parameter Estimation with Frequency and Coherence Weighting}

% 2g
\subsection{Derivation of Transfer Function with Velocity Feedback}

% 2h
\subsection{Parameter Estimation of Transfer Function with Velocity Feedback}

% 2i
\subsection{SEM of Estimated Parameters}

% 2j
\subsection{Parameters Dependent on Background Torque}

% 2k
\subsection{VAF of Segments}

% 2l
\subsection{Use of High-Pass Filters}


\end{document}
