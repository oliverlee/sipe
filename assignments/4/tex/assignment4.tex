\documentclass[11pt,a4paper]{article}
% See geometry.pdf to learn the layout options. There are lots.
\usepackage{geometry}
\geometry{left = 2.5cm, right = 2.5cm, top = 2.5cm, bottom = 2.5cm}
% Activate to begin paragraphs with an empty line rather than an indent
\usepackage[parfill]{parskip}
\usepackage[table]{xcolor}
\usepackage{courier,multirow,fancyhdr}
\usepackage{float,amsmath,graphicx,framed,subfiles}
\usepackage{amssymb,subcaption,textcomp,listings}
\usepackage{booktabs,epstopdf,caption,units}
\usepackage[framed,numbered]{matlab-prettifier}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
% hyperref should be loaded last
\usepackage[
    colorlinks=true,
    pdfencoding=auto,
    pdfauthor={Oliver Lee},
    pdftitle={WB2301-5 System Identification and Parameter Estimation:
    Assignment 4 - Optimization & Real Data Analysis},
    pdftex
]{hyperref}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\mcode}[1]{
    \lstinline[
        style=Matlab-editor,
        basicstyle=\mlttfamily,
    ]{#1}
}

\frenchspacing

\pagestyle{fancy}
\fancyhead{}
\fancyhead[R]{Lee}
\fancyhead[L]{Assignment 4}


\title{WB2301-5 System Identification and Parameter Estimation \\
Assignment 4 - Optimization \& Real Data Analysis}
\author{Oliver Lee}
\date{\today}
\graphicspath{ {images/} }

\begin{document}
\maketitle

\section{Optimization Techniques}

% 1a
\subsection{Optimization with Grid and Gradient Search}
Using a model $ \hat{y}(t) $ assumed to be:
\begin{equation*}
    \hat{y}(t) = a\cos(\frac{bt}{2}) + b\sin(\frac{at}{2})
\end{equation*}
and parameters $ a,\: b \in [0, 10] $. To find the optimal values of $a$ and
$b$, the error function $e$ to minimize is defined as:
\begin{equation*}
    e = (y(t) - \hat{y}(t))^T (y(t) - \hat{y}(t))
\end{equation*}
where $y(t),\: \hat{y}(t) \in \mathbb{R}^{n \times 1} $ and $n$ is the number
of elements of the considered time vector.

\autoref{fig:optsurf} shows the error calculation for parameters $a, b$ with a
resolution of $0.1$ (a 2-d grid) as well as multiple pathways from the use of
gradient search with different initial conditions. All initial conditions are
chosen randomly with a uniform distribution from the search space, except for
the given condition $a = 5, \: b = 5$. As can be seen, poor choices in initial
conditions result in convergence at local minima not equal to the global
minimum. In fact, for the given problem and the randomly chosen initial
conditions, most do not converge at the global minimum.  If the error function
surface (or hypersurface for a general optimization problem) is not known a
priori (i.e. grid search was not performed before use of gradient search), it
is possible to execute a number of gradient searches which only return a local
minimum without knowing that the global minimum exists elsewhere in the search
space.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{optsurf.eps}
    \caption{Error Function Surface with Gradient Search Pathways}
    \label{fig:optsurf}
\end{figure}

% 1b
\subsection{Advantages and Disadvantages of Various Optimization Techniques}
\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
%        \multicolumn{5}{|c|}{Performance of Optimization Techniques} \\
%        \hline
        \nonumber & \shortstack[c]{estimated\\parameters (a, b)} &
            \shortstack[c]{error\\residual} &
            \shortstack[c]{number of\\iterations} &
            \shortstack[c]{calculation\\time} \\
        \hline
        Grid Search & $ (5.8,\: 3.5) $ & $ 8.4835 $ & $ 10201 $ &
            $ 0.9455 $ s \\
        Gradient Search & $ (5.97,\: 3.53) $ & $ 5.3066 \times 10^{-17} $ &
            $ 8 $ & $ 0.6253 $ s \\
        Genetic Algorithm & $ (5.97,\: 3.53) $ & $ 2.2417 \times 10^{-7} $ &
            $ 100 $ & $ 4.4695 $ s \\
        \hline
    \end{tabular}
    \caption{Performance of Optimization Techniques}
    \label{tab:optperf}
\end{table}
In \autoref{tab:optperf} gradient search uses an initial condition of $ (5,\:
5) $. Performance of gradient search with different initial conditions will
result in small variations of estimated parameters, error residual, number of
iterations, and calculation time unless the initial condition is poorly choosen
and the search finds a different local minimum. Perfomance in the genetic
algorithm (GA) will also vary slightly in difference instances due to
randomness associated with genetic operations although the perfomance values
can be expected to be of the same order. Refer to \autoref{fig:optsurf} for
examples of poorly chosen initial conditions resulting in a different local
minimum with gradient search. \autoref{tab:optperf} shows that grid search
results in a large number of searches and does find the global minimum,
although the precision of the estimated parameters is limited by the resolution
of the grid used. Both gradient search and the genetic algorithm find the same
minimum, with higher precision. Gradient search is able to do quickly, with
fewest function iterations and lowest computation time. The GA is also able to
find the global minimum is fewer function iterations than grid search (orders
of magnitude lower) but total computation time is higher due to creation of
candidate solutions and selection/genetic operations used in creating
subsequent generations. Note that the number of iterations for GA is the number
of generations; the number of function evaluations is equal to product of
number of iterations and population size. For this problem, the population size
is $50$ giving a total number of function evaluations of $5000$. Although the
GA optimization takes longer in this
problem, larger and more complex optimization problems may have jacobian and
hessian matrices that are more expensive to compute (in time and memory) for
gradient based methods and GA may faster in some cases.

Advantages and disadvantages for the three optimization techniques can be
summarized below:
\begin{itemize}
    \item Grid Search
        \begin{itemize}
            \item Will determine global minimum (with given grid resolution)
            \item Astronomical number of function evalutaions (dependent on
                grid resolution)
        \end{itemize}
    \item Gradient Search
        \begin{itemize}
            \item Very fast convergence near optimum
            \item Dependent on initial conditions
        \end{itemize}
    \item Genetic Algorithm
        \begin{itemize}
            \item Does not need to compute function gradients
            \item Slow convergence near optimum
        \end{itemize}
\end{itemize}

% 1c
\subsection{``Improvements'' for Grid Search}
The estimate for grid search can be improved by increasing the resolution of
the search grid, that is, increasing the resolution for the candidate solutions
of $a$ and $b$. While this is simple to implement, a 10x increase in resolution
for both $a$ and $b$ increases the number of function evaluations by a factor
of 100. This is expensive computationally, although grid search is easily
parallized and may be a feasible solution if resources are available.

% 1d
\subsection{Options of Genetic Algorithms}
Genetic Algorithms have a number of options that affect performance of
optimization.
\begin{itemize}
    \item\mcode{PopulationSize}- Size of the population. A population size
        that is too large can result in slow convergence due to computational
        cost associated with large amounts of data. A size that is too small
        can also result in slow convergence due to limited search range as a
        result of too few genetic samples. If elitist selection and converge is
        poorly tuned, a small population size can also result in premature
        convergence.
    \item\mcode{EliteCount}- The positive integer specifying how many
        individuals survive in the next generation. With zero parent solutions
        surviving in subsequent generations, there may be a loss of good
        solutions. With too many, the ability to search the rest of the
        parameter space is reduced resulting more generations necessary for
        convergence or convergence in a local minimum.
    \item\mcode{CrossoverFraction}- The fraction of the population of
        subsequent generations created by crossover of parent solutions from
        the previous generation. A fraction that is too large can lead to
        premature convergence; the amount of children selected from mutation
        will not be large enough to explore other areas of the parameter space
        before convergence in a local minimum due to crossover/elitist
        selection. A fraction that is too small can lead to low genetic
        variation, resulting to slow convergence as a result of high mutation
        rate.
    \item\mcode{PopInitRange}- The range of parameter values for the initial
        population. A poorly formed initial range can result in an increase in
        generations required for convergence as more time is necessary to span
        the search space through crossover and mutation. If information
        regarding areas where optimal solutions exist, the PopInitRange can be
        formuated so that the initial population begins in those areas.
    \item\mcode{Generations}- The maximum number of iterations before the
        algorithm halts. If chosen too small, the algorithm may halt before
        convergence.  The algorithm stops wonce convergence is reached, however
        poor fomulation for convergence could lead to unnecessary and time
        consuming iterations.
\end{itemize}

% 1e
\subsection{Combination of Grid and Gradient Search}
Grid and gradient search can be combined to robustly by reducing the resolution
of grid search and using each point as an initial condition for gradient
search. As gradient search has fast convergence, the increase in function
evaluations due to gradient search is smaller than the decrease in function
evaluations due to the reduction in grid resolution. With the number of
iterations of each gradient search on the order of 10 and the decrease of
resolution by 10 for each $a$ and $b$, the number of function evaluations of
the combined search is an order of magnitude smaller than grid search alone.

Using the search function decribed in \autoref{lst:combsearch} to optimize
parameters $a$ and $b$, we find the performance metrics as described in
\autoref{tab:combperf}. The pathways of the gradient searches are shown in
\autoref{fig:combsearch} using a resolution of $1$ for $a$ and $b$ for the grid
resolution. While the global minimum is found, we can see visually that some of
the pathways converge at local minima and not the global minimum and show the
sensitivity of gradient search to the initial condition. While total
computation time is quite large compared to both grid search and a single
instance of gradient search (see \autoref{tab:optperf}), it is comparable to
GA. Number of iterations is the sum of the gradient search iterations for all
initial conditions and is equal to the total number of function evaluations. As
expected, the number of iterations is an order of magnitude less than simple
grid search. The parameter estimates match the values for gradient search (with
a good initial condition) and GA.

\begin{table}
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
%        \multicolumn{5}{|c|}{Performance of Optimization Techniques} \\
%        \hline
        \nonumber & \shortstack[c]{estimated\\parameters (a, b)} &
            \shortstack[c]{error\\residual} &
            \shortstack[c]{number of\\iterations} &
            \shortstack[c]{calculation\\time} \\
        \hline
        Combined Search & $ (5.97,\: 3.53) $ & $ 1.8310 \times 10^{-28} $ &
            $ 1948 $ & $ 4.2724 $ s \\
        \hline
    \end{tabular}
    \caption{Performance of Combined Grid and Gradient Search}
    \label{tab:combperf}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{combsearch.eps}
    \caption{Gradient Search Pathways from Combined Search}
    \label{fig:combsearch}
\end{figure}

\newpage
\lstset{
    caption=Combined Grid and Gradient Search Function,
    captionpos=b,
    label={lst:combsearch},
}
\lstinputlisting[
    style=Matlab-editor,
    basicstyle=\footnotesize\mlttfamily,
]{../combinedsearch.m}

\section{Real Data Analysis}

% 2a
\subsection{Perturbation and System Response Data}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{wristdata.eps}
    \caption{Angle Perturbation Input and Wrist Output Torque}
    \label{fig:wristdata}
\end{figure}
As stated in the question description, experiments involving the human wrist
use small angle perturbations about a neutral position and measure the wrist
torque of a human subject. The input and output data are presented in
\autoref{fig:wristdata}.


% 2b
\subsection{Coherence of Whole Dataset}
Reasons that the coherence of the whole dataset decreases compared to
individual segments are:
\begin{itemize}
    \item \textbf{Nonlinearity of the system} - The system is nonlinear. Since
        each segment uses only small angle perturbations about a given point,
        we can approximate behavior with a linear system for each segment. For
        small enough changes about an operating point of a system, the first
        order term of the Taylor expansion of the system will dominate. This
        process is known as \textbf{linearization} and is commonly used to
        analyze the behavior of a nonlinear system about a given point.
        However, if we combine all segments and analyze the whole data,
        coherence will decrease as each individual segment was about different
        operating points and the data can be viewed as the combination of
        different linear systems.
    \item \textbf{Time variance of the system} - A human response can be viewed
        as a time variant system as humans are subject to things such as
        fatigue, boredom, etc, and cannot be expected to react in an identical
        fashion to stimuli over the course of a long experiment. Thus if the
        system varies over the course of the different segments, which result
        in poor coherence.
\end{itemize}

% 2c
\subsection{Validity of LTI identification}
If there are indications that the system and nonlinear and time varying, than
application of linear time invariant identification techniques will result in
poor system identification.

% 2d
\subsection{Dynamic Characteristics and Parameter Estimation}
A mass, spring, damper system is represented by the second-order transfer
function:
\begin{equation*}
    H(s) = \frac{1}{Ms^2 + Bs + K}
\end{equation*}
where $M$ is the mass, $B$ is the damping coefficient, and $K$ is the spring
constant. At low frequencies, the $K$ term dominates the other terms in the
denominator of the system transfer function, and the spring parameter can be
estimated by comparing the frequency response to $1/K$. At high frequencies,
the $Ms^2$ term in the denominator dominates, and the magnitude of the
frequency response can be used to estimate the mass. Frequencies in between the
high and low ranges can be used to determine to the damping parameter $B$ after
estimating $M$ and $K$.

% 2e
\subsection{Estimation of Mass, Spring, Damper Parameters}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{mckfrf.eps}
    \caption{Frequency Response and Coherence for Wrist Data in Perturbed
        Frequency Range}
    \label{fig:mckfrf}
\end{figure}
The estimated mass, spring, and damper parameters do not match well in both the
gain and phase plots in the perturbed frequency range ($2$ and $20$ Hz) and are
shown in \autoref{fig:mckfrf}. The dotted lines with markers display the
frequency response of the measured data and dashed lines display the frequency
response of the mass-spring-damper system using estimated parameters. Segments
are shown in \autoref{fig:wristdata} by vertical bars and the different
segments can be identified by the plotted color.

% 2f
\subsection{Parameter Estimation with Frequency and Coherence Weighting}
Limiting to the error function to the range of perturbed frequencies ($2$ -
$20$ Hz) and by using frequency and coherence weighing in the error function
for parameter estimation results in a better parameter estimates; the improved
model has a better fit with the measured data. The frequency response of the
model with improved estimates is displayed in \autoref{fig:mckfrf}. Comparison
of the error residual between the models and measured data is presented in
\autoref{tab:mckerr}, and uses the sum of squares of the frequency and
coherence weighted error for perturbed frequencies to compare the fit of the
two parameter estimates.

\begin{table}[hb]
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        \nonumber & segment 1 & segment 2 & segment 3 & segment 4 \\
        \hline
        initial estimate & $15.6188$ & $34.1516$ & $22.9896$ & $28.6137$ \\
        improved estimate & $0.4966$ & $0.4267$ & $0.3800$ & $0.3365$ \\
        \hline
    \end{tabular}
    \caption{Unweighted Error Residual of Parameter Estimates for Wrist Data}
    \label{tab:mckerr}
\end{table}

% 2g
\subsection{Derivation of Transfer Function with Velocity Feedback}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{clvelfb.png}
    \caption{Human Wrist System with Velocity Feedback}
    \label{fig:clsys}
\end{figure}
Using the block diagram of the system with velocity feedback, as shown in
\autoref{fig:clsys}, we can derive the transfer function of the closed loop
system. Starting with definitions of intermediate signals $Y(s)$, $E_1(s)$, and
$E_2(s)$:
\begin{equation*}
    Y(s) = \frac{1}{Ms}E(s) \qquad E_1(s) = BY(s) + E_2(s) \qquad
    E_2(s) = K_v e^{-t_d s} H_{act} Y(s) + KX(s)
\end{equation*}
Although not specified explictly in \autoref{fig:clsys}, the delay block is
given by transfer function $ e^{-t_d s} $, where $t_d$ is the specified delay.
We can then write an expression for $E(s)$:
\begin{align*}
    E(s) &= T(s) - E_1(s) = T(s) - (BY(s) + E_2(s)) \\
         &= T(s) - (BY(s) + K_v e^{-t_d s} H_{act} Y(s) + KX(s)) \\
         &= T(s) - (B + K_v e^{-t_d s} H_{act}) Y(s) - KX(s)
\end{align*}
Substituting the relation betwen $E(s)$ and $Y(s)$, we have:
\begin{align*}
    Ms Y(s) &= T(s) - (B + K_v e^{-t_d s} H_{act}) Y(s) - KX(s) \\
    (Ms + B + K_v e^{-t_d s} H_{act}) Y(s) &= T(s) - KX(s)
\end{align*}
From the block diagram, we relate $Y(x)$ to $X(s)$ by:
\begin{equation*}
    X(s) = \frac{1}{s} Y(s)
\end{equation*}
Replacing $Y(s)$ in the expression of the closed loop system:
\begin{equation*}
    (Ms + B + K_v e^{-t_d s} H_{act}) s X(s) = T(s) - KX(s)
\end{equation*}
The resultant transfer function is:
\begin{equation*}
    \frac{X(s)}{T(s)} = \frac{1}{Ms^2 + (B + K_v e^{-t_d s} H_{act}) s  + K}
\end{equation*}

% 2h
\subsection{Parameter Estimation of Transfer Function with Velocity Feedback}
Plots of the improved 3 parameter estimate $(M, B, K)$ and 6 parameter estimate
$(M, B, K, K_v, t_d, w)$ are shown in \autoref{fig:6parfrf}. The improved 3
parameter estimate is plotted as a dashed line and the 6 parameter estimate
which includes feedback gain $K_v$ and time delay $t_d$ of velocity feedback
and cut-off frequency $w$ of the activation filter in addition to the $M$, $B$,
$K$ parameters of the improved estimate.  Coherence is not shown as it is the
same as \autoref{fig:mckfrf}. A table of the error residual between the models
and measured data is presented in \autoref{tab:6parerr}, and again uses the sum
of squares and of the frequency and coherence weighted error in the range of
perturbed frequencies for calculation of the error residual.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{6parfrf.eps}
    \caption{Frequency Response and Coherence for Wrist Data in Perturbed
        Frequency Range for Different Parameter Vectors}
    \label{fig:6parfrf}
\end{figure}

\begin{table}[hb]
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        \nonumber & segment 1 & segment 2 & segment 3 & segment 4 \\
        \hline
        3 param. est. $(M, B, K)$ &
            $0.4966$ & $0.4267$ & $0.3800$ & $0.3365$ \\
        6 param. est. $(M, B, K, K_v, t_d, w)$ &
            $0.1091$ & $0.1272$ & $0.1171$ & $0.0873$ \\
        \hline
    \end{tabular}
    \caption{Unweighted Error Residual of Different Parameter Vector Estimates
        for Wrist Data}
    \label{tab:6parerr}
\end{table}

% 2i
\subsection{SEM of Estimated Parameters}

% 2j
\subsection{Parameters Dependent on Background Torque}

% 2k
\subsection{VAF of Segments}

% 2l
\subsection{Use of High-Pass Filters}


\end{document}
